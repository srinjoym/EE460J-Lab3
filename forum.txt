We looked on the forum for inspiration when looking for the best method to perform our regression.  One that particularly stood out was a post by user MeiChengShih that detailed his stacking of 15 different models to optimize his result, and he showed that it significantly increased his score.  Looking forward to our final project and Kaggle competition, we will definitely consider such comprehensive stacking in our methodology.  He also detailed his use of outlier detection to improve his model.  He did this by finding a set of outliers to remove from his training set.  He then retrained his model using this new set and got a significantly better result.  Had the scope of this lab been greater, we may have implemented something like this.  Another example of this can be found in a post by user Andy Harless in which he exemplifies the importance of taking simple, logarithmic averages in order to improve your score. Finally, we were surprised in general at the wealth of knowledge contained in the posts.  Many people openly shared their thinking in regards to the competition, and in reading through them we were given a more realistic look at possible ways that we could take our future tasks. 
